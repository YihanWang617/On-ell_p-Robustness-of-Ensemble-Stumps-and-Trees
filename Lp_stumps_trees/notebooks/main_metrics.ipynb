{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "# Plots for experiments"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "%load_ext autoreload\n%autoreload 2\n\nimport os\nos.chdir(\"../\")\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport data\nimport utils\n\n%matplotlib inline\n"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Model (depth\u003d4): 2019-08-11 14:28:04 dataset\u003dbreast_cancer weak_learner\u003dtree model\u003drobust_bound n_train\u003d-1 n_trials_coord\u003d10 eps\u003d0.300 max_depth\u003d4 lr\u003d0.01\niter: 46/150  [test] err 0.73% adv_err_lb 6.57% adv_err 6.57%  adv_err_ub 6.57%  |  [valid] err 1.83% adv_err_lb 8.26% adv_err 8.26%  |  [train] err: 7.32%  adv_err: 13.73%  loss: 0.77129  (cert 2.356s, total 26.63 min)\nModel (depth\u003d4): 2019-08-11 14:28:04 dataset\u003ddiabetes weak_learner\u003dtree model\u003drobust_bound n_train\u003d-1 n_trials_coord\u003d8 eps\u003d0.050 max_depth\u003d4 lr\u003d0.2\niter: 9/150  [test] err 27.27% adv_err_lb 35.71% adv_err 35.71%  adv_err_ub 35.71%  |  [valid] err 22.13% adv_err_lb 30.33% adv_err 30.33%  |  [train] err: 21.75%  adv_err: 28.46%  loss: 0.87517  (cert 3.165s, total 26.82 min)\nModel (depth\u003d4): 2019-08-11 14:28:08 dataset\u003dcod_rna weak_learner\u003dtree model\u003drobust_bound n_train\u003d-1 n_trials_coord\u003d8 eps\u003d0.025 max_depth\u003d4 lr\u003d0.2\niter: 36/150  [test] err 6.91% adv_err_lb 21.26% adv_err 21.37%  adv_err_ub 21.37%  |  [valid] err 7.95% adv_err_lb 21.56% adv_err 21.59%  |  [train] err: 7.74%  adv_err: 21.06%  loss: 0.70865  (cert 5.511s, total 258.96 min)\n\nModel (depth\u003d4): 2019-08-11 14:28:05 dataset\u003dmnist_1_5 weak_learner\u003dtree model\u003drobust_bound n_train\u003d-1 n_trials_coord\u003d784 eps\u003d0.300 max_depth\u003d4 lr\u003d0.2\niter: 126/150  [test] err 0.25% adv_err_lb 1.33% adv_err 1.43%  adv_err_ub 1.43%  |  [valid] err 0.37% adv_err_lb 1.56% adv_err 1.60%  |  [train] err: 0.10%  adv_err: 0.70%  loss: 0.03662  (cert 0.443s, total 277.92 min)\nModel (depth\u003d4): 2019-08-11 14:28:05 dataset\u003dmnist_2_6 weak_learner\u003dtree model\u003drobust_bound n_train\u003d-1 n_trials_coord\u003d784 eps\u003d0.300 max_depth\u003d4 lr\u003d0.2\niter: 88/150  [test] err 0.70% adv_err_lb 3.77% adv_err 4.07%  adv_err_ub 4.07%  |  [valid] err 0.76% adv_err_lb 3.62% adv_err 4.13%  |  [train] err: 0.54%  adv_err: 2.36%  loss: 0.15701  (cert 0.284s, total 263.83 min)\nModel (depth\u003d4): 2019-08-11 14:28:05 dataset\u003dfmnist_sandal_sneaker weak_learner\u003dtree model\u003drobust_bound n_train\u003d-1 n_trials_coord\u003d784 eps\u003d0.100 max_depth\u003d4 lr\u003d0.2\niter: 128/150  [test] err 3.65% adv_err_lb 7.70% adv_err 8.10%  adv_err_ub 8.10%  |  [valid] err 4.00% adv_err_lb 8.63% adv_err 9.00%  |  [train] err: 2.46%  adv_err: 6.32%  loss: 0.32549  (cert 0.303s, total 350.61 min)\n\nModel (depth\u003d4): 2019-08-11 14:28:08 dataset\u003dgts_100_roadworks weak_learner\u003dtree model\u003drobust_bound n_train\u003d-1 n_trials_coord\u003d3072 eps\u003d0.031 max_depth\u003d4 lr\u003d0.01\niter: 105/150  [test] err 2.58% adv_err_lb 4.73% adv_err 4.73%  adv_err_ub 4.73%  |  [valid] err 3.74% adv_err_lb 7.31% adv_err 7.31%  |  [train] err: 2.93%  adv_err: 5.65%  loss: 0.41661  (cert 0.253s, total 415.60 min)\nModel (depth\u003d4): 2019-08-11 14:28:08 dataset\u003dgts_30_70 weak_learner\u003dtree model\u003drobust_bound n_train\u003d-1 n_trials_coord\u003d3072 eps\u003d0.031 max_depth\u003d4 lr\u003d0.01\niter: 148/150  [test] err 13.84% adv_err_lb 20.94% adv_err 21.38%  adv_err_ub 21.38%  |  [valid] err 7.26% adv_err_lb 14.64% adv_err 14.88%  |  [train] err: 5.42%  adv_err: 9.88%  loss: 0.56638  (cert 0.261s, total 528.54 min)\n\nLatex table:\n0.7 \u0026 6.6 \\\\\n27.3 \u0026 35.7 \\\\\n6.9 \u0026 21.3 \\\\\n0.2 \u0026 1.3 \\\\\n0.7 \u0026 3.8 \\\\\n3.6 \u0026 7.7 \\\\\n2.6 \u0026 4.7 \\\\\n13.8 \u0026 20.9 \\\\\n\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "sns.set(font_scale\u003d2.0)\nnp.random.seed(1)\nnp.set_printoptions(precision\u003d6, suppress\u003dTrue)\nplot_height, legend_size \u003d 10, 18\nmarker_size, line_width \u003d 4.0, 1.5\neps_format_dict \u003d {\u00270.300\u0027: \u00270.3\u0027,\n                   \u00270.050\u0027: \u00270.05\u0027,\n                   \u00270.025\u0027: \u00270.025\u0027,\n                   \u00270.100\u0027: \u00270.1\u0027,\n                   \u00270.031\u0027: \u00278/255\u0027}\n\ndatasets \u003d [\u0027breast_cancer\u0027, \u0027diabetes\u0027, \u0027cod_rna\u0027, \u0027mnist_1_5\u0027, \u0027mnist_2_6\u0027, \u0027fmnist_sandal_sneaker\u0027, \u0027gts_100_roadworks\u0027, \u0027gts_30_70\u0027, \u0027har\u0027, \u0027ijcnn1\u0027]\nmodels \u003d [\u0027plain\u0027, \u0027at_cube\u0027, \u0027robust_bound\u0027, \u0027robust_exact\u0027]\nexp_folder \u003d \u0027exps_diff_depth\u0027  # exps_diff_depth\nweak_learner \u003d \u0027tree\u0027\ntree_depth \u003d \u00274\u0027\nmodel_names \u003d utils.get_model_names(datasets, models, exp_folder, weak_learner, tree_depth)\n\nflag_plot \u003d False\nflag_latex \u003d True\nflag_n_trees_latex \u003d True if weak_learner \u003d\u003d \u0027tree\u0027 else False\nflag_pruning_stats \u003d False \n \nlatex_table, latex_str \u003d \u0027\u0027, \u0027\u0027\nfor i, model_name in enumerate(model_names):\n    dataset \u003d model_name.split(\u0027dataset\u003d\u0027)[1].split(\u0027 \u0027)[0]\n    model \u003d model_name.split(\u0027model\u003d\u0027)[1].split(\u0027 \u0027)[0]\n    eps \u003d model_name.split(\u0027eps\u003d\u0027)[1].split(\u0027 \u0027)[0]\n    max_depth \u003d model_name.split(\u0027max_depth\u003d\u0027)[1].split(\u0027 \u0027)[0]\n    print(\u0027Model (depth\u003d{}): {}\u0027.format(max_depth, model_name))\n    \n    metrics_path \u003d model_name + \u0027.metrics\u0027\n    metrics \u003d np.loadtxt(exp_folder + \u0027/\u0027 + metrics_path)\n    \n    if metrics.shape[1] \u003c 10:\n        print(\u0027An old model encountered! Just skipping.\u0027)\n        continue\n    \n    # needed for plots\n    iters \u003d metrics[:, 0]\n    test_errs, test_adv_errs \u003d metrics[:, 1], metrics[:, 3]\n    train_errs, train_adv_errs \u003d metrics[:, 5], metrics[:, 6]\n    train_losses \u003d metrics[:, 7]\n    valid_errs, valid_adv_errs_lb, valid_adv_errs \u003d metrics[:, 8], metrics[:, 9], metrics[:, 10]\n    \n    # Model selection is done\n    if model \u003d\u003d \u0027plain\u0027: \n        iter_to_print \u003d np.argmin(valid_errs)\n    elif model in [\u0027at_cube\u0027, \u0027robust_bound\u0027, \u0027robust_exact\u0027]:\n        # note that `da_uniform` models are mostly taken from first iterations (unless one takes them by TE)\n        # iter_to_print \u003d np.argmin((valid_errs + valid_adv_errs)/2)\n        iter_to_print \u003d np.argmin(valid_adv_errs)\n    else:\n        raise ValueError(\u0027wrong model name\u0027)\n    \n    # TODO: the last entries have to be revisited; I added a time_cert_test and removed depths/n_nodes before pruning\n    # needed to print it directly or for latex table\n    last_iter, n_iter_done, time_total \u003d int(metrics[iter_to_print, 0]), len(metrics[:, 0]), metrics[-1, 12]\n    test_err, test_adv_err_lb, test_adv_err, test_adv_err_ub \u003d metrics[iter_to_print, 1:5]\n    train_err, train_adv_err, train_loss \u003d metrics[iter_to_print, 5:8]\n    valid_err, valid_adv_err_lb, valid_adv_err, valid_adv_err_ub \u003d metrics[iter_to_print, 8:12]\n    time_cert \u003d metrics[iter_to_print, 13]\n\n    test_str \u003d \u0027iter: {}/{}  [test] err {:.2%} adv_err_lb {:.2%} adv_err {:.2%}  adv_err_ub {:.2%}\u0027.format(\n        last_iter, n_iter_done, test_err, test_adv_err_lb, test_adv_err, test_adv_err_ub)\n    valid_str \u003d \u0027[valid] err {:.2%} adv_err_lb {:.2%} adv_err {:.2%}\u0027.format(\n        valid_err, valid_adv_err_lb, valid_adv_err)\n    train_str \u003d \u0027[train] err: {:.2%}  adv_err: {:.2%}  loss: {:.5f}\u0027.format(\n        train_err, train_adv_err, train_loss)\n    pruning_str \u003d \u0027\u0027\n    if flag_pruning_stats:\n        d_before, d_after, nodes_before, nodes_after \u003d metrics[:iter_to_print+1, 13:17].mean(0)\n        pruning_str \u003d \u0027 | depth {:.2f}-\u003e{:.2f} nodes {:.2f}-\u003e{:.2f}\u0027.format(d_before, d_after, nodes_before, nodes_after)\n    print(\u0027{}  |  {}  |  {} {} (cert {:.3f}s, total {:.2f} min)\u0027.format(test_str, valid_str, train_str, pruning_str, \n                                                                        time_cert, time_total/60))\n    # form the latex table\n    if flag_latex:\n        if model \u003d\u003d \u0027plain\u0027:\n            latex_str +\u003d \u0027{} \u0026 {} \u0026  \u0027.format(data.dataset_names_dict[dataset], eps)\n        if weak_learner \u003d\u003d \u0027stump\u0027:\n            latex_str +\u003d \u0027{:.1f} \u0026 {:.1f} \u0026 {:.1f}\u0027.format(\n            test_err*100, test_adv_err*100, test_adv_err_ub*100)\n        else:\n            latex_str +\u003d \u0027{:.1f} \u0026 {:.1f} \u0026 {:.1f}\u0027.format(\n            test_err*100, test_adv_err_lb*100, test_adv_err_ub*100)\n        \n        if flag_n_trees_latex:  # add the number of trees\n            latex_str +\u003d \u0027 \u0026 {}\u0027.format(last_iter)\n        \n        # if the last column of a block\n        if weak_learner \u003d\u003d \u0027stump\u0027 and model \u003d\u003d \u0027robust_exact\u0027 or \\\n            weak_learner \u003d\u003d \u0027tree\u0027 and model \u003d\u003d \u0027robust_bound\u0027:\n            curr_row_final \u003d utils.finalize_curr_row(latex_str, weak_learner, flag_n_trees_latex)\n            latex_table +\u003d curr_row_final\n            latex_str \u003d \u0027\u0027  # re-initialize to an empty string\n        else:\n            latex_str +\u003d \u0027 \u0026 \u0027  \n    \n    if flag_plot:\n        plot_name_short \u003d \u0027{}-{}\u0027.format(dataset, model)\n        plot_name_long \u003d \u0027dataset\u003d{}-model\u003d{}-iter\u003d{}\u0027.format(dataset, model, last_iter)\n        fig, axs \u003d plt.subplots(1, 3, figsize\u003d(3*plot_height, plot_height)) # sharex\u003dTrue, sharey\u003dTrue\n    \n        axs[0].plot(iters, test_errs, label\u003d\u0027test error\u0027, linestyle\u003d\u0027solid\u0027, linewidth\u003dline_width, marker\u003d\u0027o\u0027, markersize\u003dmarker_size)\n        axs[0].plot(iters, test_adv_errs, label\u003d\u0027test adv error\u0027, linestyle\u003d\u0027solid\u0027, linewidth\u003dline_width, marker\u003d\u0027o\u0027, markersize\u003dmarker_size)\n        axs[0].plot(iters, valid_errs, label\u003d\u0027valid error\u0027, linestyle\u003d\u0027solid\u0027, linewidth\u003dline_width, marker\u003d\u0027o\u0027, markersize\u003dmarker_size)\n        axs[0].plot(iters, valid_adv_errs, label\u003d\u0027valid adv error\u0027, linestyle\u003d\u0027solid\u0027, linewidth\u003dline_width, marker\u003d\u0027o\u0027, markersize\u003dmarker_size)\n        axs[0].set_yticklabels([\u0027{:.0%}\u0027.format(x) for x in axs[0].get_yticks()])\n        axs[0].set_xlabel(\u0027iteration\u0027)\n        axs[0].set_ylabel(\u0027test error\u0027)\n        # prec \u003d 1 if np.round(test_adv_errs.max() - test_errs.min(), 1) !\u003d 0.0 else 3\n        # y_min, y_max \u003d test_errs.min().round(prec), test_adv_errs.max().round(prec)\n        # axs[0].set_yticks(np.arange(y_min, y_max, (y_max - y_min) / 10))\n        axs[0].grid(which\u003d\u0027both\u0027, alpha\u003d0.5, linestyle\u003d\u0027--\u0027)\n        axs[0].legend(loc\u003d\u0027best\u0027, prop\u003d{\u0027size\u0027: legend_size})\n        axs[0].set_title(plot_name_short)\n        \n        axs[1].plot(iters, train_adv_errs, label\u003d\u0027train error\u0027, linestyle\u003d\u0027solid\u0027, linewidth\u003dline_width, marker\u003d\u0027o\u0027, markersize\u003dmarker_size)\n        axs[1].set_yticklabels([\u0027{:.0%}\u0027.format(x) for x in axs[1].get_yticks()])\n        axs[1].set_xlabel(\u0027iteration\u0027)\n        axs[1].set_ylabel(\u0027training error\u0027)\n        # prec \u003d 1 if np.round(test_adv_errs.max() - train_adv_errs.min(), 1) !\u003d 0.0 else 3\n        # y_min, y_max \u003d train_adv_errs.min().round(prec), train_adv_errs.max().round(prec)\n        # axs[1].set_yticks(np.arange(y_min, y_max, (y_max - y_min) / 10))\n        axs[1].grid(which\u003d\u0027both\u0027, alpha\u003d0.5, linestyle\u003d\u0027--\u0027)\n        axs[1].legend(loc\u003d\u0027best\u0027, prop\u003d{\u0027size\u0027: legend_size})\n        axs[1].set_title(plot_name_short)\n        \n        axs[2].plot(iters, train_losses, label\u003d\u0027train loss\u0027, linestyle\u003d\u0027solid\u0027, linewidth\u003dline_width, marker\u003d\u0027o\u0027, markersize\u003dmarker_size)\n        # axs[2] \u003d sns.lineplot(iters, train_losses, linewidth\u003dline_width, \n        #                       marker\u003d\u0027o\u0027, markersize\u003dmarker_size, color\u003d\"black\")\n        axs[2].set_title(plot_name_short)\n        axs[2].set_xlabel(\u0027iteration\u0027)\n        axs[2].set_ylabel(\u0027training loss\u0027)\n        # prec \u003d 1 if np.round(train_losses.max() - train_losses.min(), 1) !\u003d 0.0 else 3\n        # y_min, y_max \u003d train_losses.min().round(prec), train_losses.max().round(prec)\n        # axs[2].set_yticks(np.arange(y_min, y_max, (y_max - y_min) / 10))\n        axs[2].grid(which\u003d\u0027both\u0027, alpha\u003d0.5, linestyle\u003d\u0027--\u0027)\n        axs[2].legend(loc\u003d\u0027best\u0027, prop\u003d{\u0027size\u0027: legend_size})\n        axs[2].set_title(plot_name_short)\n    \n        plt.savefig(\u0027plots/{}.pdf\u0027.format(plot_name_long), bbox_inches\u003d\u0027tight\u0027)\n    if weak_learner \u003d\u003d \u0027stump\u0027 and i % 3 \u003d\u003d 2:\n        print()\n    if weak_learner \u003d\u003d \u0027tree\u0027 and i % 3 \u003d\u003d 2:\n        print()\n\nif flag_latex:\n    # Global post-processing of the latex table\n    latex_table \u003d latex_table.replace(\u0027100.0\u0027, \u0027100\u0027)  # to save some width in the table \n    for eps_orig in eps_format_dict:\n        latex_table \u003d latex_table.replace(eps_orig, eps_format_dict[eps_orig])\n    \n    print()\n    print(\u0027Latex table:\u0027)\n    print(latex_table)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "    \n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}